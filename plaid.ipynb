{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd99170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ir_datasets\n",
      "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (4.12.3)\n",
      "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
      "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir_datasets)\n",
      "  Downloading lxml-5.3.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (4.66.6)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir_datasets)\n",
      "  Downloading lz4-4.4.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3 (from ir_datasets)\n",
      "  Downloading zlib_state-0.1.9.tar.gz (9.5 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ijson>=3.1.3 (from ir_datasets)\n",
      "  Downloading ijson-3.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
      "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pyarrow>=16.1.0 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from ir_datasets) (19.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/columbia/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (2024.8.30)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ijson-3.3.0-cp39-cp39-macosx_11_0_arm64.whl (57 kB)\n",
      "Downloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
      "Downloading lxml-5.3.2-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.4-cp39-cp39-macosx_11_0_arm64.whl (189 kB)\n",
      "Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: warc3-wet-clueweb09, zlib-state, cbor\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=760efc3ea2b81a88f5b6a7766e036488f8ebd40a9a0343b2196bc0497a0f2c86\n",
      "  Stored in directory: /Users/luigiliu/Library/Caches/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
      "  Building wheel for zlib-state (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zlib-state: filename=zlib_state-0.1.9-cp39-cp39-macosx_11_0_arm64.whl size=9711 sha256=856df32f267328b6f7f58ce3503fdd9c369046ce37ab041c1352f2860c538335\n",
      "  Stored in directory: /Users/luigiliu/Library/Caches/pip/wheels/97/bf/43/053e61846731c976e798709c2ece17fa566888b983dc3aaeed\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-macosx_11_0_arm64.whl size=19429 sha256=f30b2f0a2282a14447a7731e0a363c8cd390f0c79babb946b6d6f68b94934190\n",
      "  Stored in directory: /Users/luigiliu/Library/Caches/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
      "Successfully built warc3-wet-clueweb09 zlib-state cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, lz4, lxml, inscriptis, ir_datasets\n",
      "Successfully installed cbor-1.0.0 ijson-3.3.0 inscriptis-2.6.0 ir_datasets-0.5.10 lxml-5.3.2 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26346109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set dimensions\n",
    "# d = 128        # Embedding dimension\n",
    "# k = 1000       # Number of centroids\n",
    "# num_docs = 10000\n",
    "# tokens_per_doc = 20\n",
    "# tokens_per_query = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69872b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of MS MARCO\n",
    "dataset = load_dataset(\"ms_marco\", \"v2.1\", split=\"train[:2000]\")\n",
    "passages = []\n",
    "queries = []\n",
    "for ex in dataset:\n",
    "    passage_text = \" \".join(ex['passages'][\"passage_text\"])\n",
    "    passages.append(passage_text)\n",
    "    if len(queries) <= 5:\n",
    "        queries.append(ex['query'])\n",
    "\n",
    "# Load tokenizer + encoder (MiniLM)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name).eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53423ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated. The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science. Essay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade. The Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 â€¦ 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves. versions of each volume as well as complementary websites. The first websiteâ€“The Manhattan Project: An Interactive Historyâ€“is available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security The Manhattan Project. This once classified photograph features the first atomic bomb â€” a weapon that atomic scientists had nicknamed Gadget.. The nuclear age began on July 16, 1945, when it was detonated in the New Mexico desert. Nor will it attempt to substitute for the extraordinarily rich literature on the atomic bombs and the end of World War II. This collection does not attempt to document the origins and development of the Manhattan Project. Manhattan Project. The Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers. Nuclear physicist Robert Oppenheimer was the director of the Los Alamos Laboratory that designed the actual bombs. The Army component of the project was designated the In June 1942, the United States Army Corps of Engineersbegan the Manhattan Project- The secret name for the 2 atomic bombs. One of the main reasons Hanford was selected as a site for the Manhattan Project's B Reactor was its proximity to the Columbia River, the largest river flowing into the Pacific Ocean from the North American coast.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6fcd269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-to-vector extractor\n",
    "def encode_text(texts, max_length=64):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True,\n",
    "                       return_tensors=\"pt\", max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder(**inputs)\n",
    "    # Use token embeddings (excluding special tokens)\n",
    "    token_embeddings = outputs.last_hidden_state  # [batch, seq, hidden]\n",
    "    attention_mask = inputs.attention_mask.bool()\n",
    "    all_vecs = []\n",
    "    for i in range(len(texts)):\n",
    "        mask = attention_mask[i]\n",
    "        vecs = token_embeddings[i][mask][1:-1]  # exclude [CLS], [SEP]\n",
    "        all_vecs.append(vecs.numpy())\n",
    "    return all_vecs  # List of [n_tokens, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d11691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encode all documents (token-level vectors)\n",
    "doc_vecs = encode_text(passages[:1000])  # list of [n_tokens, d]\n",
    "\n",
    "# Flatten into a big token matrix\n",
    "flattened_tokens = np.vstack(doc_vecs)\n",
    "\n",
    "# Step 3.1: Learn centroids (k-means)\n",
    "k = 256\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, batch_size=1024, random_state=42)\n",
    "kmeans.fit(flattened_tokens)\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids /= np.linalg.norm(centroids, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8f0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert document tokens to centroid IDs\n",
    "def assign_centroids(token_vecs, centroids):\n",
    "    norms = np.linalg.norm(token_vecs, axis=1, keepdims=True)\n",
    "    token_vecs = token_vecs / norms\n",
    "    sims = np.dot(token_vecs, centroids.T)\n",
    "    return np.argmax(sims, axis=1)  # [n_tokens]\n",
    "\n",
    "doc_centroid_ids = [assign_centroids(vecs, centroids) for vecs in doc_vecs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ba271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top passages for Q0: [\"When Australian scientist Ruben Meerman lost 30 pounds last year, one question kept bugging him: Where did the fat go? The answer might seem obvious: It was burned up, as we say â€” which implies that it was transformed into heat or energy. The researchers chose to follow the path of these atoms when leaving the body. They found that when 10 kg of fat were oxidized, 8.4 kg were converted and excreted as carbon dioxide (CO2) via the lungs, and 1.6 kg became water (H20). In order for 10 kg of human fat to be oxidized, the researchers calculated that 29 kg of oxygen must be inhaled. Exercise also increases the oxidation of fat, which then leaves your body via your lungs, in the form of carbon dioxide, and your bodily fluids, in the form of water. Whatâ€™s not so complex however, is how to optimize your metabolismâ€”even if you donâ€™t understand the exact mechanisms involved. The research conducted by a team at UNSW Science in Sydney calculated exactly what happens to our fat when we shed kilos, and revealed that doctor's leading theories are wrong - we donâ€™t convert our missing mass into heat or energy, we breathe it out. If you eat less than you exhale, youâ€™ll lose weight. If youâ€™re trying to shed pounds, donâ€™t worry too much about all of this talk of carbon atoms and oxidation. Balancing the calories you eat with the calories you expend is a sound strategy. In fact, itâ€™s what Meerman did when he trimmed down last year. â€œI was drinking three cappuccinos made with full cream milk each day, and Iâ€™d hit 40 years of age,â€ he says. â€œYour metabolism slows a bit as you get older so you canâ€™t consume as many calories and expect to stay slim. 1 When you lose weight, you exhale 84 percent of the lost fat in the form of carbon dioxide. The remaining 16 percent is excreted as water via bodily fluids.  By substituting one hour of sedentary lounging with one hour of moderate exerciseâ€”to increase your respiratory rateâ€”your metabolic rate is increased sevenfold. Breathe deeply, Australian research shows that itâ€™s going to take a lot of exhaling to get rid of that excess fat. FIONA MACDONALD. 17 DEC 2014. Despite societyâ€™s obsession with weight loss, a study has revealed that, surprisingly, most health professionals donâ€™t actually know what happens to fat when we â€œlose itâ€. Consider intermittent fasting: If youâ€™re insulin/leptin resistant and/or are overweight, boost your body's fat-burning potential by incorporating intermittent fasting. This is one of the most powerful approaches to reverse insulin resistance. It is only necessary to do until your insulin resistance resolves. Breathe deeply, Australian research shows that itâ€™s going to take a lot of exhaling to get rid of that excess fat. Despite societyâ€™s obsession with weight loss, a study has revealed that, surprisingly, most health professionals donâ€™t actually know what happens to fat when we â€œlose itâ€. To lose weight, you need to break down those triglycerides to access their carbon. The results showed that in order to completely breakdown 10kg of human fat, we need to inhale 29 kg of oxygen (and somewhere along the way, burn 94,000 calories). This reaction produces 28 kg of CO2 and 11 kg of water.\", \"The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated. The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science. Essay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade. The Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 â€¦ 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves. versions of each volume as well as complementary websites. The first websiteâ€“The Manhattan Project: An Interactive Historyâ€“is available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security The Manhattan Project. This once classified photograph features the first atomic bomb â€” a weapon that atomic scientists had nicknamed Gadget.. The nuclear age began on July 16, 1945, when it was detonated in the New Mexico desert. Nor will it attempt to substitute for the extraordinarily rich literature on the atomic bombs and the end of World War II. This collection does not attempt to document the origins and development of the Manhattan Project. Manhattan Project. The Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers. Nuclear physicist Robert Oppenheimer was the director of the Los Alamos Laboratory that designed the actual bombs. The Army component of the project was designated the In June 1942, the United States Army Corps of Engineersbegan the Manhattan Project- The secret name for the 2 atomic bombs. One of the main reasons Hanford was selected as a site for the Manhattan Project's B Reactor was its proximity to the Columbia River, the largest river flowing into the Pacific Ocean from the North American coast.\", 'In doing so, the belief that a physical, Newtonian material universe that was at the very heart of scientific knowing was dropped, and the realization that matter is nothing but an illusion replaced it. Scientists began to recognize that everything in the Universe is made out of energy. Ryan Biddulph Says: The most freeing aspect of being human is the fact we have a god-like part of us which molds our reality. The most confining aspect of being human is the fact we have a god-like part of us which molds our reality ğŸ˜‰. It really is that simple, difficult. Conversely, it would be wrong to relegate our experience to the world of illusion. It is very real, the only reality we know. If I kick a boulder my foot hurts. The solidness of the stone is real in my experience; so is the pain. The illusion comes when we confuse the image in our mind with the thing-in-itself. One great example that illustrates the role of consciousness within the physical material world (which we know not to be so physical) is the double slit experiment. This experiment has been used multiple times to explore the role of consciousness in shaping the nature of physical reality. Once we realize we are creating our own reality, we begin to see how we are truly manifesting our lives. Each day we will observe more and as Oprah says, she believes everything follows Newtonâ€™s 3rd Law Of Motion: â€œFor Every Action, There Is An Equal And Opposite Reactionâ€œ. Reality is the state of things as they actually exist, rather than as they may appear or might be imagined. Reality includes everything that is and has been, whether or not it is observable or comprehensible. A still broader definition includes that which has existed, exists, or will exist. What does it mean that our physical material reality isnâ€™t really physical at all? It could mean a number of things, and concepts such as this cannot be explored if scientists remain within the boundaries of the only perceived world existing, the world we see. The dictionary says that an illusion is a misleading image. So an illusion is something that has a particular reality to it, itâ€™s just that this reality is a deceptive reality. Yet it seems to me that many people mistakenly think of an illusion as something that does not exist at all. You hear this kind of thing all the time. By contrast existence is often restricted solely to that which has physical existence or has a direct basis in it in the way that thoughts do in the brain. Reality is often contrasted with what is imaginary, illusory, delusional, (only) in the mind, dreams, what is false, what is fictional, or what is abstract. In Indian philosophy these two realities are sometimes referred to as the Absolute and the Relative. The Absolute is the underlying reality. It does not change according to who is experiencing it. It is, as it is, an independent reality. The Relative is the reality we observe, the reality generated in our minds.', \"The White House responded to Donald Trump keeping 1,000 jobs in the US by dropping a huge fact bomb that showed just how little Trump has accomplished compared to President Obama. The Hill reported that Press Secretary Josh Earnest said that the White House was pleased that the jobs were staying, then he added: FUN FACT: More net jobs have been created under Obama than both Bushes combined. The economy under President Obama seems difficult to defend â€” unless you put it in context. Here are the facts: When I took office, businesses were laying off 800,000 Americans a month. Today, our businesses are hiring 200,000 Americans a month. The White House responded to Donald Trump keeping 1,000 jobs in the US by dropping a huge fact bomb that showed just how little Trump has accomplished compared to President Obama. â€œIf [Trump] is successful in doing that 804 more times, then he will meet the record number of manufacturing jobsâ€ created during Obamaâ€™s eight years in office. Earnest went further, saying that the roughly 800,000 jobs he cited were new jobs created under Obamaâ€™s watch. Dec. 1, 2016. (The White House) During a press briefing on Dec. 1, 2016, White House Press Secretary Josh Earnest said Mr. Trump would have to make 804 more announcements just like [the Carrier deal] to equal the standard of jobs in the manufacturing sector that were created under President Obama. Dec. 1, 2016. There were 805,000 manufacturing jobs that werenâ€™t just protected or saved, but actually created while President Obama was in office. So President Obama has set a high standard, and President-elect Trump can meet that standard if this Carrier deal is completed in the way that he expects that it will be. More net jobs have been created under Obama â€” 5,142,000 as of the August jobs report â€” than under George H.W. Bush â€” 2,637,000 â€” and George W. Bush â€” 1,282,000 â€” combined, according to the Federal Reserve Bank of St. Louis. You should rightfully point out that crediting jobs created to a president exclusively is stupid. White House Press Secretary Josh Earnest said that 805,000 manufacturing jobs have been created since President Barack Obama has been in office. In fact, there has been a net loss of 303,000 manufacturing jobs since January 2009. Earnest made the statement during a press briefing on Nov. 30. (For the record, the 10-million-jobs-added figure Obama cited is just private-sector jobs and excludes much of Obama's first year in office.) But we wondered if, in the past six years, U.S. job growth has surpassed other leading economies by such a wide margin. We've now created more than 10 million. The unemployment rate's come down faster than we could have anticipated. Just to give you some perspective, Bob, we've created more jobs in the United States than every other advanced economy combined since I came into office.. By many metrics, the United States has bounced back from the recession better than other advanced countries. There were 805,000 manufacturing jobs that weren't just protected or saved, but actually created while President Obama was in office. So President Obama has set a high standard, Earnest said. Trump and Vice President-elect Mike Pence are visting the Carrier plant on Thursday.\", \"Put together, the United States constitutes roughly 40 percent of the world's military expenditures. For the period 2010â€“14, the Stockholm International Peace Research Institute (SIPRI) found that the United States was the world's largest exporter of major arms, accounting for 31 per cent of global shares. Fort Bragg is west of Fayetteville, North Carolina. It has a population of 238,646 that includes 52,280 active-duty soldiers. It covers 163,000 acres in area. Fort Campbell, on the border of Kentucky and Tennessee, has the second largest population of any U.S. military base, including 234,914 inhabitants. The American military has been. viewed as a form of national service, an occupation, a profession, a work-. place, a calling, an industry, and a set. of internal labor markets.1 Military. service has touched most American. families; nearly 26 million Americans. living today have served in the mili-. Mady Wechsler Segal is Distinguished Scholar-Teacher, professor of sociology, associate direc-. tor of the Center for Research on Military Organization, and faculty affiliate in the Womenâ€™s. Studies Program and in the School of Public Policy at the University of Maryland. Military Active-Duty Personnel, Civilians by State. Numbers of U.S. military service members vary by state, driven mostly by workforce levels at large bases. There were a total of 1.3 million active duty military and more than 800,000 reserve forces as of 2016, according to Defense Department data. Total active duty personnel for the five armed service were 471,397 for the Army, 326,276 for the Navy, 309,682 for the Air Force, 183,917 for the Marine Corps and 39,084 for the Coast Guard. The U.S. military is the world's second largest, after China's People's Liberation Army, and has troops deployed around the globe. From 1776 until September 2012, a total of 40 million people have served in the United States Armed Forces. Fort Bragg, Fort Campbell and Fort Hood are the largest U.S. military bases by population according to figures from 2013. Joint Base Lewis-McChord and Fort Benning are the next largest. It currently has 247,000 active personnel with an additional 57,900 in reserve. Japan also has 1,595 aircraft, the world's fifth largest air force, and 131 ships. Japan's military is limited by a peace clause in the constitution that makes it illegal for the country to have an offensive army. They consist of the Army, Marine Corps, Navy, Air Force, and Coast Guard. The President of the United States is the military's overall head, and helps form military policy with the U.S. Department of Defense (DoD), a federal executive department, acting as the principal organ by which military policy is carried out. Research on Military Organization, faculty associate in the Maryland Population Research. Center, and faculty affiliate in School of Public Policy at the University of Maryland. He has. published widely on military manpower, personnel, and operational issues.\"]\n"
     ]
    }
   ],
   "source": [
    "# Encode queries\n",
    "query_vecs = encode_text(queries)\n",
    "\n",
    "# For each query, compute centroid-based retrieval\n",
    "def compute_scores_for_query(q_vec):\n",
    "    q_vec = q_vec / np.linalg.norm(q_vec, axis=1, keepdims=True)\n",
    "    S_cq = centroids @ q_vec.T  # [k, |q|]\n",
    "\n",
    "    scores = []\n",
    "    for doc in doc_centroid_ids:\n",
    "        doc_scores = S_cq[doc]  # [len(doc), |q|]\n",
    "        max_sim = np.max(doc_scores, axis=0)  # [|q|]\n",
    "        scores.append(np.sum(max_sim))\n",
    "    return scores\n",
    "\n",
    "# Example: score for the first query\n",
    "scores_q0 = compute_scores_for_query(query_vecs[0])\n",
    "top_docs = np.argsort(scores_q0)[-5:][::-1]\n",
    "print(\"Top passages for Q0:\", [passages[i] for i in top_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63a90933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the MSMARCO data usage agreement found at <http://www.msmarco.org/dataset.aspx>\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.dev.tsv\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.dev.tsv: [00:00] [1.20MB] [1.77MB/s]\n",
      "[INFO] If you have a local copy of https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz, you can symlink it here to avoid downloading it again: /Users/luigiliu/.ir_datasets/downloads/c177b2795d5f2dcc524cf00fcd973be1\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz: [00:02] [18.9MB] [8.61MB/s]\n",
      "                                                                                                   \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Insufficient disk space: /Users/luigiliu/.ir_datasets/msmarco-passage/collectionandqueries.tar.gz requires 1.1GB but only 251.6MB is available (806.1MB more needed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m msmarco\u001b[38;5;241m.\u001b[39mqueries_iter():\n\u001b[1;32m     16\u001b[0m     query_texts[q\u001b[38;5;241m.\u001b[39mquery_id] \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m msmarco\u001b[38;5;241m.\u001b[39mdocs_iter():\n\u001b[1;32m     18\u001b[0m     doc_texts[d\u001b[38;5;241m.\u001b[39mdoc_id] \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/formats/tsv.py:93\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/formats/tsv.py:28\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_idx]\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctxt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menter_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/contextlib.py:448\u001b[0m, in \u001b[0;36m_BaseExitStack.enter_context\u001b[0;34m(self, cm)\u001b[0m\n\u001b[1;32m    446\u001b[0m _cm_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(cm)\n\u001b[1;32m    447\u001b[0m _exit \u001b[38;5;241m=\u001b[39m _cm_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m\n\u001b[0;32m--> 448\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_cm_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_push_cm_exit(cm, _exit)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/fileio.py:78\u001b[0m, in \u001b[0;36mCache.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/fileio.py:69\u001b[0m, in \u001b[0;36mCache.verify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streamer\u001b[38;5;241m.\u001b[39mstream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m---> 69\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose() \u001b[38;5;66;03m# close file before move... Needed because of Windows\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mmove(f\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m-\u001b[39m pos  \u001b[38;5;66;03m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleftover \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleftover \u001b[38;5;241m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(output)] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/datasets/msmarco_passage.py:81\u001b[0m, in \u001b[0;36mFixEncoding.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Find sequences of up to 4 characters that contain a suspicious character.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# We'll attempt to interpret these as latin1 characters and then decode them back to UTF8.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# With this technique, we get 100% matches with MS MARCO QnA passages (which do not have this encoding issue)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# This approach is more than twice as fast as using ftfy\u001b[39;00m\n\u001b[1;32m     76\u001b[0m regexes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(...\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|..\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.|.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m..|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...)\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     78\u001b[0m     re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(..\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m..)\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     79\u001b[0m     re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.)\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     80\u001b[0m ]\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streamer\u001b[38;5;241m.\u001b[39mstream() \u001b[38;5;28;01mas\u001b[39;00m stream, \\\n\u001b[1;32m     82\u001b[0m      _logger\u001b[38;5;241m.\u001b[39mpbar_raw(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixing encoding\u001b[39m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     83\u001b[0m      \u001b[38;5;66;03m# NOTE: codecs.getreader is subtly broken here; it sometimes splits lines between special characters (and it's unclear why)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m     85\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(line))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/fileio.py:96\u001b[0m, in \u001b[0;36mTarExtract.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mExitStack() \u001b[38;5;28;01mas\u001b[39;00m ctxt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streamer\u001b[38;5;241m.\u001b[39mstream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# IMPORTANT: open this file in streaming mode (| in mode). This means that the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;66;03m# content need not be written to disk or be fully read.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         tarf \u001b[38;5;241m=\u001b[39m ctxt\u001b[38;5;241m.\u001b[39menter_context(tarfile\u001b[38;5;241m.\u001b[39mopen(fileobj\u001b[38;5;241m=\u001b[39mstream, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m tarf:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/download.py:290\u001b[0m, in \u001b[0;36mDownload.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/download.py:260\u001b[0m, in \u001b[0;36mDownload.path\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    257\u001b[0m Path(download_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size_hint:\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_disk_free\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_size_hint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mirror \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmirrors:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/columbia/lib/python3.9/site-packages/ir_datasets/util/__init__.py:234\u001b[0m, in \u001b[0;36mcheck_disk_free\u001b[0;34m(target_path, required_size, message)\u001b[0m\n\u001b[1;32m    232\u001b[0m required_size_fmt \u001b[38;5;241m=\u001b[39m format_file_size(required_size)\n\u001b[1;32m    233\u001b[0m free_size_fmt \u001b[38;5;241m=\u001b[39m format_file_size(free_size)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    235\u001b[0m     target_path\u001b[38;5;241m=\u001b[39mtarget_path,\n\u001b[1;32m    236\u001b[0m     required_size\u001b[38;5;241m=\u001b[39mrequired_size,\n\u001b[1;32m    237\u001b[0m     required_size_fmt\u001b[38;5;241m=\u001b[39mrequired_size_fmt,\n\u001b[1;32m    238\u001b[0m     missing_size\u001b[38;5;241m=\u001b[39mmissing_size,\n\u001b[1;32m    239\u001b[0m     missing_size_fmt\u001b[38;5;241m=\u001b[39mmissing_size_fmt,\n\u001b[1;32m    240\u001b[0m     free_size\u001b[38;5;241m=\u001b[39mfree_size,\n\u001b[1;32m    241\u001b[0m     free_size_fmt\u001b[38;5;241m=\u001b[39mfree_size_fmt))\n",
      "\u001b[0;31mValueError\u001b[0m: Insufficient disk space: /Users/luigiliu/.ir_datasets/msmarco-passage/collectionandqueries.tar.gz requires 1.1GB but only 251.6MB is available (806.1MB more needed)"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load MS MARCO passage ranking dev dataset\n",
    "msmarco = ir_datasets.load(\"msmarco-passage/dev\")\n",
    "\n",
    "# Map query_id â†’ relevant passage_ids\n",
    "qrels = {}\n",
    "for qrel in msmarco.qrels_iter():\n",
    "    if qrel.relevance > 0:\n",
    "        qrels.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
    "\n",
    "# Map query_id â†’ text, doc_id â†’ text\n",
    "query_texts = {}\n",
    "doc_texts = {}\n",
    "for q in msmarco.queries_iter():\n",
    "    query_texts[q.query_id] = q.text\n",
    "for d in msmarco.docs_iter():\n",
    "    doc_texts[d.doc_id] = d.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping: text â†’ doc_id (approximate match)\n",
    "text_to_doc_id = {v: k for k, v in doc_texts.items()}\n",
    "\n",
    "# For one query\n",
    "query_id = list(query_texts.keys())[0]\n",
    "relevant_ids = qrels[query_id]\n",
    "\n",
    "# Get PLAID top passages for this query\n",
    "top_passages = [passages[i] for i in top_docs]\n",
    "top_doc_ids = [text_to_doc_id.get(p.strip(), None) for p in top_passages]\n",
    "\n",
    "# Check how many are relevant\n",
    "hits = [doc_id for doc_id in top_doc_ids if doc_id in relevant_ids]\n",
    "recall_at_k = len(hits) / len(relevant_ids)\n",
    "\n",
    "print(\"Relevant doc IDs:\", relevant_ids)\n",
    "print(\"Retrieved doc IDs:\", top_doc_ids)\n",
    "print(\"Correctly retrieved:\", hits)\n",
    "print(f\"Recall@{len(top_doc_ids)} = {recall_at_k:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94295774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba3188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f383a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4d2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd7e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
